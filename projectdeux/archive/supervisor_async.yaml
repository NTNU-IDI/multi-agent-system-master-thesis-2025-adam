scenarios:
  - name: "supervisor_async_test"
    system_type: "generic"
    execution_type: "asynchronous"
    problem: "Tell and evaluate jokes"
    goal: "Test async supervisor workflow with joke-telling"
    agents:
      - name: "Supervisor"
        role: "supervisor"
        system_prompt: |
          You are planning a project to tell and evaluate jokes. Decide how many comedians to create (e.g., 2 or 3) and specify distinct types of jokes for each (e.g., puns, one-liners, knock-knock jokes). Define agents with 'name', 'role', 'system_prompt'. For each comedian, create a task in the 'task_sequence' with 'task_name' like 'Tell a [joke type]', 'agent_name' as the comedian's name, 'instruction' like 'Tell a [joke type] about {topic}', 'params' {'topic': 'run_params.topic'}, and 'queue' 'joke_queue_{run_id}' where {run_id} ensures unique queues per run. Then, include an evaluation task assigned to the 'Critic' agent with 'task_name' 'Evaluate Jokes', 'agent_name' 'Critic', 'instruction' 'Evaluate these jokes: \n{jokes}\nPick the best one based on humor.', 'params' {'jokes': 'context.all_jokes'}, and 'queue' 'evaluation_queue_{run_id}'. Ensure 'context.all_jokes' collects all joke outputs. Return JSON with 'agents' (array) and 'task_sequence'. Example for two comedians:
          {
            "agents": [
              {"name": "Comedian1", "role": "comedian", "system_prompt": "You are a comedian who tells puns."},
              {"name": "Comedian2", "role": "comedian", "system_prompt": "You are a comedian who tells one-liners."}
            ],
            "task_sequence": [
              {"task_name": "Tell a pun", "agent_name": "Comedian1", "instruction": "Tell a pun about {topic}", "params": {"topic": "run_params.topic"}, "queue": "joke_queue_{run_id}"},
              {"task_name": "Tell a one-liner", "agent_name": "Comedian2", "instruction": "Tell a one-liner about {topic}", "params": {"topic": "run_params.topic"}, "queue": "joke_queue_{run_id}"},
              {"task_name": "Evaluate Jokes", "agent_name": "Critic", "instruction": "Evaluate these jokes: \n{jokes}\nPick the best one based on humor.", "params": {"jokes": "context.all_jokes"}, "queue": "evaluation_queue_{run_id}"}
            ]
          }
      - name: "Critic"
        role: "critic"
        system_prompt: "Evaluate all jokes and pick the best one based on humor."
    run_params:
      topic: "technology"
