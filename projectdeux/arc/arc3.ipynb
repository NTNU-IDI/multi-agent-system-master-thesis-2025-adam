{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple\n",
      "Requirement already satisfied: numpy in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: faiss-cpu in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: torch in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (0.19.1)\n",
      "Requirement already satisfied: transformers in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (4.45.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: packaging in /Users/adamds/.local/lib/python3.8/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/adamds/.local/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/adamds/.pyenv/versions/3.8.12/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy faiss-cpu torch torchvision transformers sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import faiss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training tasks loaded: 400\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the ARC dataset\n",
    "arc_dataset_path = 'data/training'  # Adjust this path\n",
    "\n",
    "# Function to load ARC tasks\n",
    "def load_arc_tasks(task_dir):\n",
    "    tasks = []\n",
    "    for filename in os.listdir(task_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(task_dir, filename), 'r') as f:\n",
    "                task = json.load(f)\n",
    "                tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "# Load training tasks\n",
    "train_tasks = load_arc_tasks(arc_dataset_path)\n",
    "print(f\"Total training tasks loaded: {len(train_tasks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique symbols: 10\n"
     ]
    }
   ],
   "source": [
    "# Define maximum grid size\n",
    "MAX_GRID_SIZE = 30  # You can adjust this based on the dataset\n",
    "\n",
    "# Function to preprocess grids\n",
    "def preprocess_grid(grid, max_size=MAX_GRID_SIZE):\n",
    "    height = len(grid)\n",
    "    width = len(grid[0])\n",
    "    # Initialize a grid filled with zeros\n",
    "    processed_grid = np.zeros((max_size, max_size), dtype=np.int64)\n",
    "    # Copy the original grid into the processed grid\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            processed_grid[i, j] = grid[i][j]\n",
    "    return processed_grid\n",
    "\n",
    "# Build a set of all colors/symbols used in the dataset\n",
    "symbols = set()\n",
    "for task in train_tasks:\n",
    "    for example in task['train']:\n",
    "        for row in example['input']:\n",
    "            symbols.update(row)\n",
    "        for row in example['output']:\n",
    "            symbols.update(row)\n",
    "\n",
    "symbol_to_int = {symbol: idx for idx, symbol in enumerate(sorted(symbols))}\n",
    "int_to_symbol = {idx: symbol for symbol, idx in symbol_to_int.items()}\n",
    "num_symbols = len(symbol_to_int)\n",
    "print(f\"Total unique symbols: {num_symbols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_symbols=num_symbols):\n",
    "        super(GridEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_symbols, 16)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # Reduces spatial dimensions by half\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # Reduces spatial dimensions by half again\n",
    "        )\n",
    "        # Initialize fc later after computing output size\n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # x: (batch_size, grid_h, grid_w, embedding_dim)\n",
    "        x = x.permute(0, 3, 1, 2)  # x: (batch_size, channels, grid_h, grid_w)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Initialize fc layer if not already done\n",
    "        if self.fc is None:\n",
    "            self.fc = nn.Linear(x.size(1), embedding_dim).to(x.device)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletGridDataset(Dataset):\n",
    "    def __init__(self, tasks):\n",
    "        self.samples = []\n",
    "        for task in tasks:\n",
    "            for example in task['train']:\n",
    "                input_grid = preprocess_grid(example['input'])\n",
    "                output_grid = preprocess_grid(example['output'])\n",
    "                self.samples.append((input_grid, output_grid))\n",
    "        \n",
    "        # Create negatives (shuffle outputs)\n",
    "        self.negatives = [output for _, output in self.samples]\n",
    "        random.shuffle(self.negatives)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor, positive = self.samples[idx]\n",
    "        negative = self.negatives[idx]\n",
    "        # Convert to tensors\n",
    "        anchor = torch.tensor(anchor, dtype=torch.long)\n",
    "        positive = torch.tensor(positive, dtype=torch.long)\n",
    "        negative = torch.tensor(negative, dtype=torch.long)\n",
    "        return anchor, positive, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m negative \u001b[38;5;241m=\u001b[39m negative\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m anchor_emb \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m positive_emb \u001b[38;5;241m=\u001b[39m encoder(positive)\n\u001b[1;32m     26\u001b[0m negative_emb \u001b[38;5;241m=\u001b[39m encoder(negative)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 24\u001b[0m, in \u001b[0;36mGridEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize fc layer if not already done\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[43membedding_dim\u001b[49m)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_dim' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate the dataset and data loader\n",
    "triplet_dataset = TripletGridDataset(train_tasks)\n",
    "triplet_loader = DataLoader(triplet_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the encoder and move it to the device\n",
    "encoder = GridEncoder().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (anchor, positive, negative) in enumerate(triplet_loader):\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        anchor_emb = encoder(anchor)\n",
    "        positive_emb = encoder(positive)\n",
    "        negative_emb = encoder(negative)\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(triplet_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grid \u001b[38;5;129;01min\u001b[39;00m all_input_grids:\n\u001b[1;32m     13\u001b[0m         grid_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(grid, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m         embedding \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(embedding\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert embeddings to numpy array\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mGridEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# x: (batch_size, channels, grid_h, grid_w)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers(x)\n\u001b[0;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Collect all input grids\n",
    "all_input_grids = []\n",
    "for task in train_tasks:\n",
    "    for example in task['train']:\n",
    "        input_grid = preprocess_grid(example['input'])\n",
    "        all_input_grids.append(input_grid)\n",
    "\n",
    "# Encode all input grids\n",
    "encoder.eval()\n",
    "embeddings = []\n",
    "with torch.no_grad():\n",
    "    for grid in all_input_grids:\n",
    "        grid_tensor = torch.tensor(grid, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        embedding = encoder(grid_tensor)\n",
    "        embeddings.append(embedding.cpu().numpy())\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings_np = np.vstack(embeddings).astype('float32')\n",
    "\n",
    "# Build the FAISS index\n",
    "embedding_dim = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings_np)\n",
    "print(f\"FAISS index built with {index.ntotal} embeddings.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
